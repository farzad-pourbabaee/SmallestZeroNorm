{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab28c8a",
   "metadata": {},
   "source": [
    "## Data Generating Process:\n",
    "\n",
    "The population regression equation is $y = F w + \\epsilon$. In the following simulations we assume $y \\in \\mathbb{R}^n$, $F \\in \\mathbb{R}^{n \\times d}$ so naturally $w \\in \\mathbb{R}^d$ and $\\epsilon \\in \\mathbb{R}^n$. \n",
    "\n",
    "Let us further assume that elements in $\\epsilon = (\\epsilon_1,\\ldots, \\epsilon_n)$ are i.i.d. with the Gaussian distribution of $\\mathcal{N}(0, \\sigma^2_{\\text{noise}})$.\n",
    "\n",
    "We also generate the elements of the design matrix $F$ by i.i.d. draws from $\\mathcal{N}(0,1)$, that is $F_{i,j} \\sim \\mathcal{N}(0,1)$ where $i\\in[n]$ and $j\\in [d]$. Subsequently, we *standardize* the columns of the factor matrix $F$.\n",
    "\n",
    "Lastly, we need to generate the *true factor loadings*, i.e., $w = (w_1,\\ldots,w_d)$. We pick a fraction of indices from $1$ to $d$ uniformly at random (called the *active* set). The size of this fraction is determined by the *sparsity level* chosen in $[0,1]$. These indices represent the non-zero values in $w$. If an index $i$ belongs to this active set of indices, then we independently draw $w_i \\sim \\mathcal{N}(0,1)$. Otherwise $i$ represents an inactive loading, and hence we independently draw $w_i \\sim \\mathcal{N}(0,\\sigma^2_{\\text{off}})$, where $\\sigma_{\\text{off}}\\ll 1$ is rather small, say $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3aac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e1e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGP:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Data generating process class\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "        n : number of observations\n",
    "        d : total number of factors\n",
    "        sparsity : fraction of factor loadings that are non-zero\n",
    "        sigmaNoise : Std of the additive i.i.d. Gaussian noise \n",
    "        sigmaInactive : Std of the inactive elements of factor loadings\n",
    "        \n",
    "        \n",
    "    Methods\n",
    "    ------------\n",
    "        sampleNoise : Generating the vector of additive noise, epsilon\n",
    "        sampleFactors : Generating the factor matrix, F\n",
    "        sampleW : Generating the factor loadings, w\n",
    "        generateY : Using the above methods, it generates the outcome y\n",
    "        \n",
    "        \n",
    "    Attributes\n",
    "    ------------\n",
    "        All the parameters plus\n",
    "        noise : returning the sampled noise vector\n",
    "        F : returning the sampled factor matrix\n",
    "        w :  returning the factor loadings\n",
    "        y : returning the outcome vector y\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=10, d=100, sparsity=0.1, sigmaNoise=0.05, sigmaInactive=0.002):\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.sparsity = sparsity\n",
    "        self.sigmaNoise = sigmaNoise\n",
    "        self.sigmaInactive = sigmaInactive\n",
    "        \n",
    "        \n",
    "    def sampleNoise(self, seedNum):\n",
    "        self.noise = norm.rvs(loc=0, scale=self.sigmaNoise, size=self.n, random_state=seedNum)\n",
    "        \n",
    "        \n",
    "    def sampleFactors(self, seedNum):\n",
    "        F = norm.rvs(loc=0, scale=1, size=(self.n, self.d), random_state=seedNum)\n",
    "        \n",
    "        # Standardizing the columns of F:\n",
    "        scaler = StandardScaler()\n",
    "        self.F = scaler.fit_transform(F)\n",
    "        \n",
    "\n",
    "    \n",
    "    def sampleW(self, seedNum):\n",
    "        np.random.seed(seedNum)\n",
    "        \n",
    "        # Finding the size of non-zero elements of w:\n",
    "        activeSize = int(self.d * self.sparsity)\n",
    "        \n",
    "        # Drawing the active and inactive indices uniformly at random from d positions:\n",
    "        active = np.random.choice(range(self.d), size=activeSize, replace=False)\n",
    "        in_active = np.array(list((set(range(self.d)) - set(active))))\n",
    "        \n",
    "        # Creating the factor loadings:\n",
    "        w = np.zeros(self.d)\n",
    "        w[active] = norm.rvs(loc=0, scale=1, size=activeSize)\n",
    "        w[in_active] = norm.rvs(loc=0, scale=self.sigmaInactive, size=self.d - activeSize)\n",
    "        self.w = w\n",
    "\n",
    "        \n",
    "    def generateY(self, *args):\n",
    "        if len(args) == 3:\n",
    "            F, w, eps = args\n",
    "            self.y = F @ w + eps\n",
    "        else:\n",
    "            self.y = (self.F @ self.w) + self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc93a89",
   "metadata": {},
   "source": [
    "## Smallest $\\ell_0$ norm extraction:\n",
    "\n",
    "We want to find the vector of weights with the smallest $\\ell_0$ norm, accepting a certain level of $\\ell_2$ training error on the observation vector:\n",
    "\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\min_w \\lVert w \\rVert_0\\, \\text{ subject to }\\, \\frac{1}{n}\\lVert y - F w \\rVert_2^2 \\leq b^2\\,.\n",
    "$$\n",
    "In this problem, $\\lVert \\cdot \\rVert_0$ is the $\\ell_0$ norm, and $b$ is the $\\ell_2$ slack. This is essentially a combinatorial problem, because the $\\lVert \\cdot \\rVert_0$ norm counts the number of non-zero elements in $w$. Instead, we use a *differentiable surrogate* for this term:\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\lVert w \\rVert_0 \\approx S(w):= d - \\sum_{k=1}^d e^{-w^2_k/2\\tau^2}\n",
    "$$\n",
    "As $\\tau$ decreases down to $0$, the approximation becomes more precise. Therefore, fixing a certain level $\\tau$ we solve the following (non-convex) minimization instead of the above program:\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\min_{w} S(w)\\, \\text{ subject to }\\, \\frac{1}{n}\\lVert y - F w \\rVert_2^2 \\leq b^2\\,.\n",
    "$$\n",
    "\n",
    "The gradient of the above objective function with respect to $w$ is\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\nabla S(w) =  \\frac{1}{\\tau^2} \\left[w_1 e^{-w^2_1/2\\tau^2}, \\ldots,w_d e^{-w^2_d/2\\tau^2} \\right]^T\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1f10f",
   "metadata": {},
   "source": [
    "## Optimization algorithm:\n",
    "\n",
    "### 1. Projection onto the feasible set (inner loop):\n",
    "\n",
    "Suppose we have a vector $w$, and we seek to project it onto the feasible set $\\mathcal{F(b)}=\\left\\{v: \\frac{1}{n}\\lVert y - F v \\rVert_2^2 \\leq b^2\\right\\}$. For this, one needs to solve the following convex minimizatin problem:\n",
    "$$\n",
    "\\normalsize\n",
    "\\min \\lVert w - v\\rVert_2^2\\, \\text{ subject to } \\lVert y - F v \\rVert_2^2 \\leq  b^2 n\\,.\n",
    "$$\n",
    "\n",
    "The associated Largrangian for this problem is\n",
    "$$\n",
    "\\normalsize\n",
    "\\mathcal{L}(v, \\lambda) = \\lVert w - v\\rVert_2^2 + \\lambda \\left(\\lVert y - F v \\rVert_2^2 - b^2 n\\right)\\,.\n",
    "$$\n",
    "\n",
    "At the optimum $(v^*, \\lambda^*)$, the necessary first order condition with respect to the primal variable $v$ is\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\nabla_v \\mathcal{L}(v^*, \\lambda^*) = 0\\, \\Rightarrow v^* = (I + \\lambda^* F^T F)^{-1}(w + \\lambda^* F^T y)\\,.\n",
    "$$\n",
    "\n",
    "The dual function is obtained by plugging the above form of $v$ into the Largrangian $\\mathcal{L}(v, \\lambda)$. Let us denote the dual function by $\\lambda \\mapsto \\Phi(\\lambda)$:\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\Phi(\\lambda) = -(w + \\lambda F^T y)^T (I + \\lambda F^T F)^{-1} (w + \\lambda F^T y) + \\lVert w \\rVert_2^2 +\\lambda (\\lVert y \\rVert_2^2 - b^2 n)\\,.\n",
    "$$\n",
    "\n",
    "Consequently, the dual problem is to maximize $\\Phi(\\lambda)$ on $\\lambda \\geq 0$. Since the *Slater's* condition holds, then one has strong duality, and the optimal value for the dual problem matches the optimal value of the primal problem. \n",
    "\n",
    "We need to find the $\\lambda^*$ that minimizes $-\\Phi(\\lambda)$ on the region $\\lambda^*\\geq 0$, that in turn gives us the optimal dual variable. Substituting that into the expression for $v^*$ gives the projection of $w$ onto $\\mathcal{F}(b)$. Minimizing $-\\Phi(\\lambda)$ is *equivalent* to\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "\\min J(\\lambda)\\, \\text{ on } \\lambda \\geq 0\\,, \\text{where } J(\\lambda):=(w + \\lambda F^T y)^T (I + \\lambda F^T F)^{-1} (w + \\lambda F^T y) - \\lambda (\\lVert y \\rVert_2^2 - b^2 n)\\,.\n",
    "$$\n",
    "\n",
    "Using a *variational* method, we can find $J'(\\lambda)$:\n",
    "\n",
    "$$\n",
    "\\normalsize\n",
    "J'(\\lambda) = -(w + \\lambda F^T y)^T (I + \\lambda F^T F)^{-1}(F^T F)(I + \\lambda F^T F)^{-1}(w + \\lambda F^T y)+2y^TF (I + \\lambda F^T F)^{-1}(w + \\lambda F^T y) - \\lVert y \\rVert_2^2 + b^2 n\\,.\n",
    "$$\n",
    "\n",
    "Given the above information, one can apply Gradient Descent with line search and backtracking to find $\\lambda^*$ and subsequently the projection of $w$ denoted by $w^+$. The projection algorithm is as follows:\n",
    "\n",
    "- initialize $\\lambda = \\lVert w \\rVert_2^2 / \\lVert y \\rVert_2^2$ and find $J'(\\lambda)$\n",
    "    \n",
    "- while $\\lvert J'(\\lambda) \\rvert / J(\\lambda) > 0.01$:\n",
    "    - Set the step size $t=1$, and find $J'(\\lambda)$\n",
    "    - *Line search + backtracking* with parameters $\\alpha_{\\text{dual}} \\in (0.01,0.3)$ and $\\beta_{\\text{dual}} \\in (0.1,0.8)$:\n",
    "    - while $J\\left(\\lambda - t J'(\\lambda)\\right) > J(\\lambda) - \\alpha_{\\text{dual}} t J'(\\lambda)^2$:\n",
    "        - update $t \\leftarrow \\beta_{\\text{dual}} t$ (i.e., depreciating step size at the rate of $\\beta_{\\text{dual}}$)\n",
    "    - update $\\lambda \\leftarrow \\lambda - t J'(\\lambda)$\n",
    "    \n",
    "    \n",
    "### 2. Optimizing the weights (outer loop):\n",
    "Now that we know how to optimally find the projection of a weight vector $w$ onto $\\mathcal{F}(b)$, we can explain the descent algorithm behind the $\\ell_0$ norm minimization:\n",
    "\n",
    "- initialize $w = F^{\\dagger}y$ and $\\tau = 2\\max_{k} |w_k|$\n",
    "- while $\\tau > \\tau_{\\text{min}}$:\n",
    "        \n",
    "    - Repeat $T$ times:\n",
    "        - update $w \\leftarrow w - \\mu \\tau^2 \\nabla S(w)$\n",
    "        - project $w$ onto the feasible set $\\mathcal{F}(b)$, hence $w \\leftarrow w^+$\n",
    "    - shrink $\\tau$ by $\\tau \\leftarrow r \\tau$ for $r \\in (0,1)$\n",
    "    \n",
    "### 3. In reality $b$ is not known:\n",
    "Observe that the prior knowledge about $b$ is not always available. All we have access to is the factor matrix $F$ and the outcomes $y$. Therefore, to go about this problem, in a recursive manner, we estimate $w$, and based on that choose a new $b$ until a convergence criterion is reached. Initially $w = F^{\\dagger}y$. Therefore, we set $b = \\lVert y - F w\\rVert_2 * \\xi$, where $\\xi>1$ is a hyperparameter that makes sure the noise is not trained. Based on this $b$, we find the new $w$ and repeat this process until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674cb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimize:\n",
    "    \"\"\"\n",
    "    Implementation of the explained algorithm (inner and outer loop)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        tauMin : minimum tau in the sequence of surrogate functions\n",
    "        T : the number of iterations for every fixed tau\n",
    "        mu : the multiplier coefficient in the gradient descent of outer loop\n",
    "        r : depreciation rate of tau\n",
    "        alpha : first parameter of the backtracking line seach in the inner loop\n",
    "        beta : second parameter of the backtracking line seach in the inner loop\n",
    "\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "        wHat : estimated w \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, tauMin=0.005, T=5, mu=2, r=0.9, alpha=0.1, beta=0.9, xi=1.4):\n",
    "        self.tauMin = tauMin\n",
    "        self.T = T\n",
    "        self.mu = mu\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.xi = xi\n",
    "    #--------------------------------------------------------------------------------------------------    \n",
    "        \n",
    "        \n",
    "    ### Implemeting the projection step (inner loop):   \n",
    "    \n",
    "    def dual(self, F, y, lam, w, b):\n",
    "        # this is the (negative) of the dual function\n",
    "        n, d = F.shape  \n",
    "        vec = w + lam * F.T @ y\n",
    "    \n",
    "        return np.dot(vec, np.linalg.inv(np.eye(d) + lam * F.T @ F) @ vec) - lam * (np.dot(y, y) - b**2 * n)\n",
    "    \n",
    "    def dual_gradient(self, F, y, lam, w, b):\n",
    "        # this method computes the gradient of dual function\n",
    "        n, d = F.shape \n",
    "        vec = w + lam * F.T @ y\n",
    "        tmp = np.linalg.inv(np.eye(d) + lam * F.T @ F) \n",
    "        \n",
    "        return -np.dot(vec, tmp @ F.T @ F @ tmp @ vec) + 2 * y.T @ F @ tmp @ vec - np.dot(y, y) + b**2 * n\n",
    "    \n",
    "    def projection(self, F, y, w, b):\n",
    "        # This function performs the projection onto the feasible region\n",
    "        # First, we find the optimal dual variable lambda:\n",
    "        d = F.shape[1]\n",
    "        lam = np.dot(w, w) / np.dot(y, y)\n",
    "        while np.abs(self.dual_gradient(F, y, lam, w, b)) / self.dual(F, y, lam, w, b) > 0.1:\n",
    "            t = 1\n",
    "            dual = self.dual(F, y, lam, w, b)\n",
    "            dualGrad = self.dual_gradient(F, y, lam, w, b)\n",
    "            while self.dual(F, y, lam - t * dualGrad, w, b) > dual - self.alpha * t * dualGrad**2:\n",
    "                t *= self.beta\n",
    "            lam -= t * dualGrad\n",
    "            \n",
    "        # Next, we find the projection of weight vector w onto the feasible region (having found lambda):\n",
    "        return np.linalg.inv(np.eye(d) + lam * F.T @ F) @ (w + lam * F.T @ y)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "            \n",
    "    ### Optimizing the weights (outer loop):\n",
    "    \n",
    "    def surrogateGradient(self, w, tau):\n",
    "        # We intentionally do NOT divide by tau^2, because in all future cases, \\\n",
    "        # the gradient vector is multiplied by this constant.\n",
    "\n",
    "        return w * np.exp(-w**2 / (2 * tau**2))\n",
    "    \n",
    "    def fit(self, F, y):\n",
    "        # Initializing w and b\n",
    "        n = y.shape[0]\n",
    "        w = np.linalg.pinv(F) @ y\n",
    "        b_prev = np.sqrt(np.sum((y - F @ w)**2) / n) * self.xi\n",
    "        b_curr = b_prev * 1.15\n",
    "        \n",
    "        while np.abs(b_curr - b_prev) / b_prev > 0.2:\n",
    "            tau = 2 * np.max(np.abs(w))\n",
    "            while tau > self.tauMin:\n",
    "                for _ in range(self.T):\n",
    "                    w -= self.mu * self.surrogateGradient(w, tau)\n",
    "                    w = self.projection(F, y, w, b_curr)\n",
    "                tau *= self.r \n",
    "            b_prev, b_curr = b_curr, np.sqrt(np.sum((y - F @ w)**2) / n) * self.xi\n",
    "            \n",
    "        self.wHat = w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ba897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
